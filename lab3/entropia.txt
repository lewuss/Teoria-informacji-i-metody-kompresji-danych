C:\Users\wojtek\AppData\Local\Programs\Python\Python311\python.exe C:\Users\wojtek\PycharmProjects\Teoria-informacji-i-metody-kompresji-danych\lab3\entropy.py 
English character entropy:
[4.288221612546937, 7.804826193654523, 10.823145027723, 13.304710511825894, 15.325894864211767]
[4.288221612546937, 3.516604581107586, 3.0183188340684763, 2.4815654841028945, 2.0211843523858732]
English word entropy:
[11.543991622407374, 17.933164298132503, 20.109623579029073, 20.594301542591545, 20.70395385269892]
[11.543991622407374, 6.3891726757251295, 2.1764592808965695, 0.48467796356247206, 0.10965231010737497]
Latin character entropy:
[4.228247555937123, 7.67837359044951, 10.50186573646578, 12.653897345843713, 14.29666150075509]
[4.228247555937123, 3.4501260345123876, 2.823492146016269, 2.152031609377934, 1.6427641549113758]
Latin word entropy:
[11.969197019815711, 16.36922747156448, 17.536114863302917, 17.92415413594181, 18.13062699042584]
[11.969197019815711, 4.400030451748767, 1.1668873917384381, 0.38803927263889193, 0.20647285448402997]
Esperanto character entropy:
[4.176788573290366, 7.516793874814476, 10.388620809576844, 12.781245463957925, 14.772755238531529]
[4.176788573290366, 3.3400053015241102, 2.8718269347623684, 2.3926246543810805, 1.9915097745736041]
Esperanto word entropy:
[11.56052798256562, 18.118205162203175, 20.602923975596433, 21.236541692016075, 21.39819945873541]
[11.56052798256562, 6.557677179637555, 2.4847188133932576, 0.6336177164196428, 0.1616577667193333]
Estonian character entropy:
[4.169833357271393, 7.676791692626328, 10.811197898613498, 13.422063593109927, 15.533506082396837]
[4.169833357271393, 3.506958335354936, 3.13440620598717, 2.610865694496429, 2.11144248928691]
Estonian word entropy:
[13.746243848864564, 19.170429715893764, 20.07516874958595, 20.191364555289006, 20.21501066968588]
[13.746243848864564, 5.4241858670292, 0.9047390336921843, 0.11619580570305743, 0.023646114396875362]
Somali character entropy:
[4.040114106637861, 7.339679742024669, 10.184050424127886, 12.558360087018249, 14.503391235097759]
[4.040114106637861, 3.2995656353868075, 2.844370682103217, 2.3743096628903633, 1.9450311480795097]
Somali word entropy:
[11.73109601722425, 17.129829491122127, 18.738450604606072, 19.14805479754527, 19.26466353823746]
[11.73109601722425, 5.398733473897877, 1.6086211134839452, 0.4096041929391987, 0.11660874069218963]
Haitian character entropy:
[4.146386120757479, 7.260247104441648, 9.533772142906047, 11.025924752572308, 12.078068350644212]
[4.146386120757479, 3.1138609836841686, 2.273525038464399, 1.4921526096662614, 1.052143598071904]
Haitian word entropy:
[8.166919763088648, 11.360036221183625, 12.671366405622033, 13.483598784384494, 14.104177092704028]
[8.166919763088648, 3.193116458094977, 1.3113301844384075, 0.8122323787624612, 0.6205783083195335]
Navajo character entropy:
[3.8749384531743107, 6.822205192461267, 9.189785942798766, 10.985050260426776, 12.326648232154737]
[3.8749384531743107, 2.947266739286956, 2.367580750337499, 1.7952643176280105, 1.3415979717279605]
Navajo word entropy:
[9.15401153063034, 13.017945012694028, 14.736750852589816, 15.635996098583226, 16.174563090102556]
[9.15401153063034, 3.863933482063688, 1.7188058398957882, 0.8992452459934093, 0.5385669915193301]

C:\Users\wojtek\AppData\Local\Programs\Python\Python311\python.exe C:\Users\wojtek\PycharmProjects\Teoria-informacji-i-metody-kompresji-danych\lab3\entropy.py
0 character entropy:
[4.273001294848433, 7.188895504541673, 9.189254874219252, 10.7285368604548, 12.167118989830595]
[4.273001294848433, 2.9158942096932403, 2.0003593696775788, 1.5392819862355491, 1.4385821293757939]
0 word entropy:
[7.748741007210299, 15.23513229345473, 19.641834110417676, 20.236841649122777, 20.248902619013364]
[7.748741007210299, 7.486391286244432, 4.406701816962945, 0.5950075387051008, 0.012060969890587359]
1 character entropy:
[4.12700632315213, 7.366156261876173, 10.227435969332028, 12.554120694278668, 14.367630676874048]
[4.12700632315213, 3.2391499387240437, 2.8612797074558545, 2.32668472494664, 1.8135099825953809]
1 word entropy:
[11.500688787312129, 16.872929669166954, 18.447666077033382, 18.955174973860814, 19.248630929878036]
[11.500688787312129, 5.372240881854825, 1.5747364078664283, 0.5075088968274315, 0.2934559560172225]
2 character entropy:
[3.993311751298641, 7.043751120254024, 9.511411163859515, 11.451183222891919, 13.153215805630433]
[3.993311751298641, 3.050439368955383, 2.467660043605491, 1.9397720590324035, 1.7020325827385143]
2 word entropy:
[8.023866431000588, 15.372488632709052, 19.154422605919365, 20.013927208696774, 20.09591715741201]
[8.023866431000588, 7.348622201708464, 3.7819339732103128, 0.8595046027774096, 0.08198994871523624]
3 character entropy:
[3.93029803165716, 7.114764460044341, 9.74265954552551, 11.766649889477344, 13.300892231909481]
[3.93029803165716, 3.1844664283871813, 2.6278950854811693, 2.0239903439518336, 1.5342423424321368]
3 word entropy:
[9.061111591600127, 15.011326738839374, 17.642130739785355, 18.906219844231924, 19.320545279056415]
[9.061111591600127, 5.950215147239247, 2.6308040009459805, 1.264089104446569, 0.4143254348244909]
4 character entropy:
[4.253809610509979, 8.482911176861945, 12.70974026472633, 16.88827556891539, 20.65440717691412]
[4.253809610509979, 4.229101566351966, 4.226829087864385, 4.17853530418906, 3.76613160799873]
4 word entropy:
[17.12967501706089, 20.57392721597078, 20.808002447384332, 20.811229090553443, 20.81123591532538]
[17.12967501706089, 3.44425219890989, 0.23407523141355213, 0.003226643169110588, 6.82477193691966e-06]
5 character entropy:
[4.4416881423123735, 7.9647862362996, 11.215406950494119, 14.049678472363485, 16.22211937356373]
[4.4416881423123735, 3.523098093987226, 3.250620714194519, 2.8342715218693666, 2.1724409012002432]
5 word entropy:
[16.50952757660461, 16.509527545743822, 16.50952751488657, 16.509527484015365, 16.50952745314629]
[16.50952757660461, -3.0860789479447703e-08, -3.085725097662362e-08, -3.0871206035953946e-08, -3.0869074407746666e-08]

sample0 - NIE - brak różnica między entropią warunkową rzędu 0, a 1
sample1 - TAK - entropia warunkowa bardzo przypomina zachowanie języka naturalnego
sample2 - NIE - brak różnica między entropią warunkową rzędu 0, a 1
sample3 - TAK - entropia warunkowa bardzo przypomina zachowanie języka naturalnego
sample4 - NIE - entropia warunkowa liter w ogóle się nie rózni przez pierwsze 4 rzędy
sample5 - NIE - entropia warunkowa zbyt gwałtowanie spada, a potem praktycznie w ogóle się nie różni